# main_step_by_step_baseline_only_safe.py
import os
import time
import re
import unicodedata
import pandas as pd
from tqdm import tqdm

from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser
from langchain_community.chat_models import ChatOllama


# --- CONFIG ---
INPUT_CSV = "normalized_cove_data.csv"
OUTPUT_CSV = "normalized_cove_data_result_baseline_only.csv"
MODELS = ["gemma3:1b"]
LLM_TEMP = 0.1
SLEEP_BETWEEN_CALLS = 0.2


# --- Utility Functions ---
def clean_text(text: str) -> str:
    try:
        if not isinstance(text, str):
            text = str(text)
    except Exception:
        text = ""
    text = unicodedata.normalize("NFC", text)
    text = re.sub(r"[^\x09\x0A\x0D\x20-\x7E√Ä-·ªπƒêƒë]", " ", text)
    text = " ".join(text.split())
    return text.strip()


# --- LLM Utility ---
def get_single_fact_answer(question: str, llm: ChatOllama):
    prompt_template = ChatPromptTemplate.from_template(
        """
        Answer the below question correctly and concisely in one very short, factual sentence.
Do not provide explanations, context, or multiple answers.
Example question 1: How often does spermatogeneis‚Äîthe production of sperm‚Äîoccur?
Example answer 1: 74 days
Example question 2: When was the first remote control tv invented?
Example answer 2: 1950

Question: {question}

Answer:
        """
    )
    chain = prompt_template | llm | StrOutputParser()
    try:
        answer = chain.invoke({"question": question})
        return answer.strip()
    except Exception as e:
        return f"LLM ERROR: {repr(e)}"


def get_normalized_question(question: str, llm: ChatOllama):
    """
    S·ª≠ d·ª•ng LLM ƒë·ªÉ l√†m r√µ c√¢u h·ªèi kh√¥ng r√µ r√†ng, gi√∫p chu·∫©n h√≥a truy v·∫•n.
    """
    prompt_template = ChatPromptTemplate.from_template(
        """
You are a question clarifier. Given the ambiguous question below, rewrite it into a clear, single-fact question. 
If the question is already clear, return the original question.

Ambiguous Question: {question}
Normalized Question: 
"""
    )
    chain = prompt_template | llm | StrOutputParser()
    try:
        normalized_q = chain.invoke({"question": question}).strip()
        return normalized_q
    except Exception:
        return question


def evaluate_answer(reference_answer: str, generated_answer: str, llm):
    if llm is None:
        return None, "LLM not initialized."
    prompt = ChatPromptTemplate.from_template(
        """
You are an expert evaluator for factual accuracy.

Given:
- Reference answer (trusted): {reference_answer}
- Model-generated answer (to evaluate): {generated_answer}

Task:
Evaluate how factually consistent and correct the generated answer is compared to the reference.
Return ONLY one integer score from 0 to 100 (higher = more accurate).

Output format:
<number>
"""
    )
    parser = StrOutputParser()
    chain = prompt | llm | parser
    try:
        response = chain.invoke({
            "reference_answer": reference_answer,
            "generated_answer": generated_answer
        }).strip()
        nums = re.findall(r'\d+', response)
        if not nums:
            raise ValueError(f"No number found in response: {response}")
        score = int(nums[0])
        score = min(max(score, 0), 100)
        return score, response
    except Exception as e:
        return None, f"ERROR: {repr(e)}"


# --- Core Processing ---
def process_data_with_evaluation_safe(df: pd.DataFrame, chain_llm: ChatOllama, eval_llm: ChatOllama,
                                      output_csv: str, model_name: str):
    df = df.rename(columns={'best_answer': 'reference_answer'}, errors='ignore')

    # B·ªï sung c√°c c·ªôt c·∫ßn thi·∫øt
    new_cols = ['model', 'baseline_answer', 'normalized_question',
                'normalized_answer', 'baseline_score', 'normalized_score']
    for col in new_cols:
        if col not in df.columns:
            df[col] = ''

    # G√°n model
    df["model"] = model_name

    # X√°c ƒë·ªãnh h√†ng c·∫ßn x·ª≠ l√Ω
    to_process_idx = [idx for idx, val in df['baseline_score'].items() if pd.isna(val) or val == '']

    if not to_process_idx:
        print("‚úÖ Kh√¥ng c√≥ h√†ng n√†o c·∫ßn x·ª≠ l√Ω ho·∫∑c ƒë√°nh gi√°.")
        return df

    print(f"üîç T√¨m th·∫•y {len(to_process_idx)} h√†ng c·∫ßn x·ª≠ l√Ω ({model_name}).")

    for i, idx in enumerate(tqdm(to_process_idx, desc=f"Model {model_name}")):
        question = str(df.at[idx, "question"]).strip()
        reference_answer = str(df.at[idx, "reference_answer"]).strip()

        # Baseline answer
        base_ans = get_single_fact_answer(question, chain_llm)
        df.at[idx, 'baseline_answer'] = clean_text(base_ans)

        # Normalized question + answer
        norm_q = get_normalized_question(question, chain_llm)
        df.at[idx, 'normalized_question'] = clean_text(norm_q)

        norm_ans = get_single_fact_answer(norm_q, chain_llm)
        df.at[idx, 'normalized_answer'] = clean_text(norm_ans)

        # Evaluate
        score_base, _ = evaluate_answer(reference_answer, base_ans, eval_llm)
        score_norm, _ = evaluate_answer(reference_answer, norm_ans, eval_llm)

        df.at[idx, 'baseline_score'] = score_base if score_base is not None else -1
        df.at[idx, 'normalized_score'] = score_norm if score_norm is not None else -1

        # --- ‚úÖ Checkpoint: ghi ngay sau m·ªói h√†ng ---
        df.to_csv(output_csv, index=False, encoding="utf-8-sig")

        print(f"[Checkpoint] Row {idx} saved ‚Üí baseline={score_base}, normalized={score_norm}")
        time.sleep(SLEEP_BETWEEN_CALLS)

    print(f"‚ú® Ho√†n th√†nh model {model_name}.")
    return df


# --- Entry Point ---
def run_batch_safe(input_csv=INPUT_CSV, output_csv=OUTPUT_CSV):
    if not os.path.exists(input_csv):
        raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file input: {input_csv}")

    df_input = pd.read_csv(input_csv)
    if "question" not in df_input.columns:
        raise ValueError("File input ph·∫£i c√≥ c·ªôt 'question'")
    if "best_answer" not in df_input.columns:
        raise ValueError("File input ph·∫£i c√≥ c·ªôt 'best_answer' (ground truth)")

    # N·∫øu file output ƒë√£ t·ªìn t·∫°i ‚Üí t·ª± ph·ª•c h·ªìi
    if os.path.exists(output_csv):
        print(f"‚ôªÔ∏è ƒêang kh√¥i ph·ª•c t·ª´ file {output_csv} ...")
        df_output = pd.read_csv(output_csv)
        # Gi·ªØ l·∫°i c√°c h√†ng ch∆∞a c√≥ ƒëi·ªÉm
        # H·ª£p nh·∫•t d·ªØ li·ªáu ƒë·∫ßu v√†o v√† ƒë·∫ßu ra c≈©
        merged = pd.merge(df_input, df_output, how="outer", on="question", suffixes=('', '_old'))

        # ∆Øu ti√™n gi·ªØ d·ªØ li·ªáu ƒë√£ c√≥ trong file k·∫øt qu·∫£ c≈©
        for col in df_output.columns:
            col_old = f"{col}_old"
            if col in merged.columns and col_old in merged.columns:
                merged[col] = merged[col].combine_first(merged[col_old])

        # X√≥a c·ªôt t·∫°m th·ªùi
        merged = merged[[c for c in merged.columns if not c.endswith('_old')]]

        df_input = merged
        print(f"‚úÖ ƒê√£ ph·ª•c h·ªìi {len(df_output)} h√†ng, ti·∫øp t·ª•c ph·∫ßn c√≤n l·∫°i.")
    else:
        print("üöÄ B·∫Øt ƒë·∫ßu m·ªõi (ch∆∞a c√≥ file k·∫øt qu·∫£).")

    # Ch·∫°y l·∫ßn l∆∞·ª£t t·ª´ng model
    for model_name in MODELS:
        print(f"\n======================")
        print(f"üîπ Running model: {model_name}")
        print(f"======================")
        try:
            chain_llm = ChatOllama(model=model_name, temperature=LLM_TEMP)
            eval_llm = ChatOllama(model=model_name, temperature=0.0)
            df_input = process_data_with_evaluation_safe(df_input, chain_llm, eval_llm,
                                                         output_csv=output_csv, model_name=model_name)
        except Exception as e:
            print(f"‚ùå Error with model {model_name}: {e}")
            continue

    print(f"\n‚úÖ Ho√†n t·∫•t to√†n b·ªô. K·∫øt qu·∫£ l∆∞u t·∫°i: {output_csv}")


if __name__ == "__main__":
    run_batch_safe()
